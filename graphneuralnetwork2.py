# -*- coding: utf-8 -*-
"""GraphNeuralNetwork2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ZnuQe-3yEowwmDT1Iqgd3cigJ3zTz30T
"""

import networkx as nx
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# Create a sample graph
G = nx.karate_club_graph()

# Plot the graph
nx.draw(G, with_labels=True)
plt.show()

# Convert graph to PyTorch tensors
edge_index = torch.tensor(list(G.edges()), dtype=torch.long).t().contiguous()
x = torch.ones(G.number_of_nodes(), 1)

print(x.shape)

class GNN(torch.nn.Module):
    def __init__(self, hidden_channels):
        super(GNN, self).__init__()
        torch.manual_seed(12345)
        self.conv1 = nn.Conv1d(34, hidden_channels, 1)
        self.conv2 = nn.Conv1d(hidden_channels, hidden_channels, 1)

    def forward(self, x, edge_index):
        x = self.conv1(x)
        x = x.relu()
        x = F.relu(self.conv2(x))
        return x

model = GNN(hidden_channels=4)
output = model(x, edge_index)

plt.hist(output.detach().numpy())
plt.title('Output Feature Distribution')
plt.show()

## This code first creates a sample graph using the karate club network from NetworkX.
# It then converts the graph to PyTorch tensors - edge_index contains the connectivity information
# and x contains the initial feature for each node.
# A simple 2-layer GNN model is defined using PyTorch nn.Conv1d layers.
# The model takes the node features x and connectivity edge_index as input.
# It passes x through two convolutional layers with ReLU activations to output node embeddings.
# The model is initialized and run on the sample graph.
# The output is plotted as a histogram to visualize the distribution of learned node embeddings.

## The key steps are:
# Represent graph as tensors
# Define convolution layers
# Pass node features through conv layers
# Output is node embeddings
# This shows the basic working of a graph neural network in PyTorch in a simple way.
# More complex implementations can have deeper models, residual connections, batch normalization etc.
# The same principle of passing features through convolutional layers on graph topology applies.

import networkx as nx
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F

# Create sample graph
G = nx.karate_club_graph()

# Plot graph
nx.draw(G, with_labels=True)
plt.show()  # Use plt.show() instead of plt.savefig()

# Features and adjacency matrix
x = torch.eye(G.number_of_nodes(), dtype=torch.float)  # Set data type to float
A = torch.tensor(nx.to_numpy_array(G), dtype=torch.float)  # Convert to tensor and set data type to float

# GNN Layer
class GNNLayer(nn.Module):
    def __init__(self, in_channels, out_channels):
        super().__init__()
        self.linear = nn.Linear(in_channels, out_channels)

    def forward(self, x, A):
        x = torch.matmul(A, x)
        x = self.linear(x)
        return x

# Create GNN
class GNN(nn.Module):
    def __init__(self, in_channels, out_channels, num_layers):
        super().__init__()
        self.layers = nn.ModuleList()
        self.layers.append(GNNLayer(in_channels, 8))
        self.layers.append(GNNLayer(8, out_channels))

    def forward(self, x, A):
        for layer in self.layers:
            x = layer(x, A)
        return x

# Train
model = GNN(34, 4, 2)
output = model(x, A)

# Plot output
plt.hist(output.detach().numpy())
plt.title('Output Feature Distribution')
plt.show()  # Use plt.show() instead of plt.savefig()

# This implements a 2-layer GNN using PyTorch modules and graph convolution.
## The key steps are:
# Create graph and get adjacency matrix
# Define GNN layer to do graph convolution
# Build model with input and output channels
# Train model on graph
# Plot output node embeddings
# The GNNLayer does message passing - multiplying the adjacency matrix with input features.
# The GNN stacks two such layers and trains it end-to-end on the graph.

"""In this next implementation:

We define a GraphConvolutionLayer class representing a single layer of the GNN.
The GNN class defines the entire GNN model architecture, consisting of two graph convolution layers with ReLU activation.
Sample data including an adjacency matrix, node features, and labels are created.
We define the loss function (binary cross-entropy) and optimizer (Adam).
The model is trained for a specified number of epochs using the training loop.
During evaluation, we calculate the accuracy of the model's predictions compared to the true labels.**bold text**
"""

import torch
import torch.nn as nn
import torch.optim as optim

class GraphConvolutionLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(GraphConvolutionLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, adj_matrix, node_features):
        output = torch.matmul(adj_matrix, node_features)
        output = self.linear(output)
        return output

class GNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GNN, self).__init__()
        self.gc1 = GraphConvolutionLayer(input_dim, hidden_dim)
        self.gc2 = GraphConvolutionLayer(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, adj_matrix, node_features):
        h1 = self.relu(self.gc1(adj_matrix, node_features))
        h2 = self.gc2(adj_matrix, h1)
        return h2

# Sample data
adj_matrix = torch.tensor([[0, 1, 0],
                            [1, 0, 1],
                            [0, 1, 0]], dtype=torch.float32)
node_features = torch.tensor([[1, 0],
                              [0, 1],
                              [1, 1]], dtype=torch.float32)
labels = torch.tensor([[0],
                       [1],
                       [0]], dtype=torch.float32)

# Model definition
input_dim = 2
hidden_dim = 16
output_dim = 1
model = GNN(input_dim, hidden_dim, output_dim)

# Loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
epochs = 1000
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(adj_matrix, node_features)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch [{epoch}/{epochs}], Loss: {loss.item()}")

# Evaluation
with torch.no_grad():
    predicted_labels = torch.round(torch.sigmoid(model(adj_matrix, node_features)))
    accuracy = (predicted_labels == labels).sum().item() / labels.numel()
    print(f"Accuracy: {accuracy}")

"""To visualize the GNN model architecture, we can use the torchviz library to create a computational graph. First, ensure you have torchviz installed:"""

!pip install torchviz

import torch
from torchviz import make_dot

# Sample data
adj_matrix = torch.tensor([[0, 1, 0],
                            [1, 0, 1],
                            [0, 1, 0]], dtype=torch.float32)
node_features = torch.tensor([[1, 0],
                              [0, 1],
                              [1, 1]], dtype=torch.float32)

# Model definition
input_dim = 2
hidden_dim = 16
output_dim = 1
model = GNN(input_dim, hidden_dim, output_dim)

# Forward pass to generate computational graph
outputs = model(adj_matrix, node_features)

# Visualize the computational graph
make_dot(outputs, params=dict(model.named_parameters()))

"""This code snippet generates a computational graph representing the forward pass of the GNN model. The make_dot function from torchviz creates a visualization of the computational graph. You can save the visualization as an image or display it directly in your Python environment.

Ensure you have Graphviz installed on your system to generate the visualizations. You can install it using your system's package manager or download it from the Graphviz website: https://graphviz.org/download/.

This visualization helps in understanding the flow of data through the layers of the GNN model and can aid in debugging and optimizing the model architecture.

Choose a simple graph data structure such as an adjacency matrix and utilize the previous GNN model for classification. Here's how we can do it:

In this implementation, we create a simple graph represented by an adjacency matrix and node features. We then define a GNN model using the provided architecture and train it for a classification task using binary cross-entropy loss. Finally, we evaluate the model's accuracy on the given dataset.

You can modify the adjacency matrix and node features to experiment with different graph structures and datasets. This implementation provides a basic framework for utilizing GNNs for graph-based tasks.
"""

import torch
import torch.nn as nn
import torch.optim as optim

class GraphConvolutionLayer(nn.Module):
    def __init__(self, input_dim, output_dim):
        super(GraphConvolutionLayer, self).__init__()
        self.linear = nn.Linear(input_dim, output_dim)

    def forward(self, adj_matrix, node_features):
        output = torch.matmul(adj_matrix, node_features)
        output = self.linear(output)
        return output

class GNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(GNN, self).__init__()
        self.gc1 = GraphConvolutionLayer(input_dim, hidden_dim)
        self.gc2 = GraphConvolutionLayer(hidden_dim, output_dim)
        self.relu = nn.ReLU()

    def forward(self, adj_matrix, node_features):
        h1 = self.relu(self.gc1(adj_matrix, node_features))
        h2 = self.gc2(adj_matrix, h1)
        return h2

# Sample data
adj_matrix = torch.tensor([[0, 1, 0],
                            [1, 0, 1],
                            [0, 1, 0]], dtype=torch.float32)
node_features = torch.tensor([[1, 0],
                              [0, 1],
                              [1, 1]], dtype=torch.float32)
labels = torch.tensor([[0],
                       [1],
                       [0]], dtype=torch.float32)

# Model definition
input_dim = 2
hidden_dim = 16
output_dim = 1
model = GNN(input_dim, hidden_dim, output_dim)

# Loss function and optimizer
criterion = nn.BCEWithLogitsLoss()
optimizer = optim.Adam(model.parameters(), lr=0.01)

# Training loop
epochs = 1000
for epoch in range(epochs):
    optimizer.zero_grad()
    outputs = model(adj_matrix, node_features)
    loss = criterion(outputs, labels)
    loss.backward()
    optimizer.step()

    if epoch % 100 == 0:
        print(f"Epoch [{epoch}/{epochs}], Loss: {loss.item()}")

# Evaluation
with torch.no_grad():
    predicted_labels = torch.round(torch.sigmoid(model(adj_matrix, node_features)))
    accuracy = (predicted_labels == labels).sum().item() / labels.numel()
    print(f"Accuracy: {accuracy}")